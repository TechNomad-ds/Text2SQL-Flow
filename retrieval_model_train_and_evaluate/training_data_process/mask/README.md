# Masked Data Construction
## Overview

This directory provides utilities to construct **masked version** of each training example’s:
- natural language question (**masked_question**)
- SQL answer (**masked_sql**)

## How masking works

### 1) `masked_sql`
Generated by `sql2skeleton`:
- **Table/column names** → `<mask>`
- **String / numeric literals** → `<unk>`

### 2) `masked_question`
Generated using schema-linking annotations (`sc_link`, `cv_link`):
- **Schema mentions** (matched table/column tokens) → `<mask>`
- **Value mentions** (matched cell values / numbers / dates) → `<unk>`

---

## Prerequisites

This pipeline relies on the dataset structure and schema-linking files produced by **DAIL-SQL**.

1. Download DAIL-SQL:
   - `git clone https://github.com/BeachWang/DAIL-SQL.git`

2. Run data preprocessing (example: Spider):
```bash
python data_preprocess.py --data_type spider
```
After preprocessing, path_data/<dataset>/enc/train_schema-linking.jsonl should exist.

## Run: Build masked dataset

Example (Spider):

```bash
python get_masked_sql.py \
  --dataset spider \
  --train_json /DAIL-SQL/dataset/spider/train_spider.json \
  --path_data /DAIL-SQL/dataset/ \
  --out train_spider_masked.json
```

### Arguments

* `--dataset`: dataset name (e.g., `spider`, `realistic`, `bird`)
* `--train_json`: input training JSON path (original examples)
* `--path_data`: DAIL-SQL `dataset/` directory

  Must contain a subdirectory named `--dataset` and have been preprocessed
* `--out`: output JSON path


## Extending to new datasets

To add support for another dataset:

1. Extend `data_preprocess.py` with a new `data_type`
2. Extend `get_masked_sql.py` to recognize the new dataset
